# üöó Vehicle Data ETL Pipeline

A clean, modular **ETL pipeline** that ingests a raw vehicles CSV, **transforms** it into analytics‚Äëready data, and **loads** it into a SQLite database for **SQL analysis** and easy extension to cloud.

---

## ‚ú® Highlights
- **End‚Äëto‚Äëend ETL**: Extract ‚Üí Transform ‚Üí Load
- **Production style**: modular code, logging, error handling
- **Feature engineering**: adds `car_age`
- **Database ready**: loads to SQLite (`vehicles.db`) with a single command
- **Analysis included**: SQL queries for common business questions
- **Cloud‚Äëfriendly**: structure easily ports to PostgreSQL / AWS RDS & Airflow

---

## üß± Architecture
```mermaid
flowchart LR
    A[Raw CSV: vehicles_dataset.csv] --> B[Extract (extract.py)]
    B --> C[Transform (transform.py)
        ‚Ä¢ clean missing values
        ‚Ä¢ normalize text
        ‚Ä¢ type casting
        ‚Ä¢ feature: car_age
        ‚Ä¢ de-dup]
    C --> D[Load (load.py) ‚Üí SQLite: data/vehicles.db (table: vehicles)]
    D --> E[Analytics: sql_queries.sql / BI / Dashboard]
```

---

## üìÇ Project Structure
```
vehicle_data_pipeline/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ vehicles_dataset.csv          # raw input
‚îÇ   ‚îî‚îÄ‚îÄ vehicles.db                   # SQLite database (created by pipeline)
‚îú‚îÄ‚îÄ extract.py                        # read & validate CSV
‚îú‚îÄ‚îÄ transform.py                      # clean, normalize, feature engineer
‚îú‚îÄ‚îÄ load.py                           # write to SQLite
‚îú‚îÄ‚îÄ main.py                           # orchestration (runs ETL)
‚îú‚îÄ‚îÄ utils.py                          # logging & DB helpers
‚îú‚îÄ‚îÄ sql_queries.sql                   # analysis queries
‚îú‚îÄ‚îÄ requirements.txt                  # python deps
‚îî‚îÄ‚îÄ README.md                         # this file
```

---

## üß™ Dataset Schema (Input)
**Rows:** ~1,000 | **Columns:** 18

| Column | Type | Notes |
|---|---|---|
| name | string | Listing title (e.g., "2024 Jeep Wagoneer Series II") |
| description | string? | Optional free text |
| make | string | Brand (e.g., Jeep, GMC, RAM) |
| model | string | Model name |
| type | string | New/Used/Certified |
| year | int | Model year |
| price | float? | Price; may be missing |
| engine | string? | Engine spec text |
| cylinders | float? | e.g., 4, 6, 8 (may be missing) |
| fuel | string? | e.g., gasoline, diesel (may be missing) |
| mileage | float? | Odometer reading; may be missing |
| transmission | string? | e.g., 8-Speed Automatic |
| trim | string? | Variant/trim |
| body | string? | e.g., SUV, Pickup Truck |
| doors | float? | e.g., 4 |
| exterior_color | string? |  |
| interior_color | string? |  |
| drivetrain | string? | e.g., Four-wheel Drive |

> `?` indicates fields that may contain missing values in the raw file.

---

## üîß Transformation Rules
- **De-duplication**: remove exact duplicate rows
- **Missing values**
  - `price` ‚Üí median of available `price`
  - `mileage` ‚Üí median of available `mileage`
  - `fuel` ‚Üí "unknown"
- **Normalization**
  - `fuel`, `transmission` ‚Üí lowercased and trimmed
- **Type casting**
  - Ensure `year` = int; `price`, `mileage` numeric
- **Feature engineering**
  - `car_age` = `current_year` ‚àí `year`

> All steps are logged with record counts before/after.

---

## ‚ñ∂Ô∏è Quickstart

### 1) Environment
```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2) Place data
Put your raw file at `data/vehicles_dataset.csv` (already included in this repo).

### 3) Run ETL
```bash
python main.py
```
This will create `data/vehicles.db` with a table named `vehicles`.

### 4) Run Analysis Queries
Use your favorite SQLite client, or:
```bash
sqlite3 data/vehicles.db < sql_queries.sql
```

---

## üß© Code Overview

### `utils.py`
- `get_logger(name)` ‚Üí standard structured logging
- `get_db_connection(db_path)` ‚Üí ensures `data/` exists and opens SQLite connection

### `extract.py`
- `extract_data(file_path)` ‚Üí reads CSV; logs shape; raises on failure

### `transform.py`
- `transform_data(df)` ‚Üí applies rules above, adds `car_age`, logs final shape

### `load.py`
- `load_data(df, table_name)` ‚Üí writes DataFrame to SQLite (`if_exists="replace"`)

### `main.py`
- `run_pipeline()` ‚Üí orchestrates Extract ‚Üí Transform ‚Üí Load

---

## üß† Analysis ‚Äì `sql_queries.sql`
```sql
-- 1) Average price by brand
SELECT make, ROUND(AVG(price), 2) AS avg_price
FROM vehicles
GROUP BY make
ORDER BY avg_price DESC;

-- 2) Top 5 most expensive cars
SELECT name, make, model, year, price
FROM vehicles
ORDER BY price DESC
LIMIT 5;

-- 3) Average mileage by fuel type
SELECT fuel, ROUND(AVG(mileage), 2) AS avg_mileage
FROM vehicles
GROUP BY fuel
ORDER BY avg_mileage DESC;

-- 4) Price vs. age (avg)
SELECT car_age, ROUND(AVG(price), 2) AS avg_price
FROM vehicles
GROUP BY car_age
ORDER BY car_age ASC;

-- 5) Most common drivetrain per brand
SELECT make, drivetrain, COUNT(*) AS cnt
FROM vehicles
GROUP BY make, drivetrain
ORDER BY cnt DESC;
```

---

## üß± Extending to Cloud / Orchestration
- **Database**: swap SQLite for PostgreSQL (local Docker or AWS RDS)
- **Orchestration**: add an **Airflow** or **Prefect** DAG to schedule daily/weekly
- **Storage**: land raw CSVs in S3/Blob/GCS; version with timestamps
- **CI/CD**: GitHub Actions to run tests + lint + pipeline on PRs
- **Serving**: build a Streamlit or Power BI dashboard on top of the DB

---

## ‚úÖ What This Project Demonstrates
- Realistic **data cleaning & standardization**
- **Feature engineering** for downstream analytics
- **Database modeling** and **SQL querying**
- **Reproducible** pipeline with clear separation of concerns

---

## üß≠ Roadmap
- [ ] Unit tests (pytest) for transform logic
- [ ] Data validation (Great Expectations)
- [ ] Dockerfile + Compose (SQLite ‚Üí Postgres)
- [ ] Airflow DAG
- [ ] Streamlit dashboard

---

## üìÑ License
MIT ‚Äì free to use and adapt.

---

## ü§ù Contributing
PRs & issues welcome! Please open an issue to discuss major changes.

---

## üôå Acknowledgements
Sample dataset provided by the project owner for educational purposes.

